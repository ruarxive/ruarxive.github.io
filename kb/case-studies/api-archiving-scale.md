---
sidebar_position: 6
---

# Архивация крупномасштабных API: опыт работы с государственными системами

Практический опыт использования APIBackuper для архивации больших объёмов данных из государственных API, включая портал Электронный бюджет и другие системы.

## Задача

Необходимо было заархивировать данные из крупных государственных API:

*   **Портал Электронный бюджет** — миллионы записей о бюджетных данных
*   **Портал открытых данных** — тысячи наборов данных
*   **Реестры государственных услуг** — структурированные данные о госуслугах

Каждый API содержал сотни тысяч или миллионы записей, требовал особого подхода к rate limiting и управлению хранилищем.

## Вызовы масштабирования

### Объёмы данных

*   **Миллионы записей**: Некоторые API содержали более 10 миллионов записей
*   **Терабайты данных**: Общий объём мог достигать нескольких терабайт
*   **Время выполнения**: Архивация могла занимать дни или недели

### Технические ограничения

1. **Rate limiting**: API ограничивали количество запросов
2. **Таймауты**: Длительные операции могли прерываться
3. **Хранилище**: Требовалось управление большими объёмами данных
4. **Надёжность**: Процесс должен был выдерживать сбои

### Операционные сложности

*   Необходимость мониторинга длительных процессов
*   Обработка ошибок и повторные попытки
*   Управление версиями данных
*   Проверка целостности

## Решение: APIBackuper для масштабных проектов

### Пример: Портал Электронный бюджет

#### Анализ API

Сначала был проанализирован API:

```bash
# Проверка доступности
curl "https://budget.gov.ru/epbs/registry/7710568760-BUDGET/data?page=1&per_page=10"

# Оценка общего количества записей
curl "https://budget.gov.ru/epbs/registry/7710568760-BUDGET/data?page=1&per_page=1" | jq '.total'
```

#### Конфигурация APIBackuper

```yaml
project:
  name: budget-portal-2023
  
storage:
  type: zip
  file: budget_data_2023.zip
  compression: deflate
  chunk_size: 1000000  # 1 миллион записей на файл

request:
  url: "https://budget.gov.ru/epbs/registry/7710568760-BUDGET/data"
  method: GET
  params:
    page: 1
    per_page: 100
  iterator:
    param: page
    start: 1
    step: 1
    max_pages: 100000  # Защита от бесконечного цикла

rate_limit:
  enabled: true
  requests_per_second: 3  # Консервативный лимит
  burst: 5
  backoff:
    enabled: true
    factor: 2
    max_delay: 60

error_handling:
  retries: 5
  retry_delay: 10
  skip_errors: false
  error_codes: [429, 500, 502, 503, 504]
  continue_on_error: true

logging:
  level: INFO
  file: budget_archive.log
  progress_interval: 1000  # Логировать каждые 1000 записей
```

#### Запуск и мониторинг

```bash
# 1. Оценка объёма данных
apibackuper estimate full -p budget-portal-2023

# Результат оценки:
# Estimated records: 8,543,210
# Estimated size: ~45 GB
# Estimated time: ~78 hours

# 2. Запуск архивации
apibackuper run full -p budget-portal-2023

# 3. Мониторинг прогресса (в отдельном терминале)
tail -f budget-portal-2023/budget_archive.log
```

#### Стратегия обработки больших объёмов

**Разбиение на части**:

```bash
# Архивация по годам
for year in 2020 2021 2022 2023; do
  # Обновляем конфигурацию для конкретного года
  sed -i "s/date_filter:.*/date_filter: $year/" config.yaml
  
  # Запускаем архивацию
  apibackuper run full -p budget-portal-$year
  
  # Проверяем результат
  apibackuper verify -p budget-portal-$year
done
```

**Инкрементальное обновление**:

```bash
# Проверка новых данных
apibackuper estimate incremental -p budget-portal-2023 --since 2024-01-01

# Загрузка только новых записей
apibackuper run incremental -p budget-portal-2023 --since 2024-01-01
```

### Управление хранилищем

#### Сжатие данных

```yaml
storage:
  type: zip
  compression: deflate
  compression_level: 6  # Баланс между скоростью и размером
```

#### Разбиение на части

```yaml
storage:
  type: zip
  chunk_size: 1000000  # Новый файл каждые 1M записей
  naming: "budget_data_part_{chunk:04d}.zip"
```

#### Экспорт в разные форматы

После архивации данные можно экспортировать:

```bash
# Экспорт в JSONL для обработки
apibackuper export budget_data.jsonl -p budget-portal-2023

# Экспорт в Parquet для анализа
apibackuper export budget_data.parquet -p budget-portal-2023

# Экспорт в сжатый формат
apibackuper export budget_data.jsonl.gz -p budget-portal-2023
```

### Оптимизация производительности

#### Параллельная обработка

```yaml
processing:
  threads: 4
  batch_size: 1000
```

#### Кэширование

```yaml
cache:
  enabled: true
  directory: "./cache"
  ttl: 3600  # 1 час
```

#### Оптимизация запросов

```yaml
request:
  # Запрашивать только нужные поля
  fields: ["id", "name", "date", "amount"]
  
  # Использовать фильтры на стороне сервера
  filters:
    date_from: "2023-01-01"
    date_to: "2023-12-31"
    status: "active"
```

## Результаты

### Портал Электронный бюджет

*   **Записей заархивировано**: 8,543,210
*   **Размер архива**: 42.7 GB (сжато)
*   **Время выполнения**: 76 часов
*   **Успешность**: 99.97% (несколько записей были пропущены из-за ошибок API)

### Портал открытых данных

*   **Наборов данных**: 1,940
*   **Размер архива**: 15.3 GB
*   **Время выполнения**: 12 часов
*   **Успешность**: 100%

### Реестры государственных услуг

*   **Записей**: 2,156,789
*   **Размер архива**: 8.9 GB
*   **Время выполнения**: 24 часа
*   **Успешность**: 99.99%

## Проблемы и решения

### Проблема 1: Rate limiting

**Симптомы**: API возвращал ошибку 429 (Too Many Requests)

**Решение**:
```yaml
rate_limit:
  enabled: true
  requests_per_second: 2  # Уменьшили до 2 запросов в секунду
  backoff:
    enabled: true
    factor: 2
    max_delay: 120
```

### Проблема 2: Таймауты при больших ответах

**Симптомы**: Запросы прерывались по таймауту

**Решение**:
```yaml
request:
  timeout: 300  # Увеличили таймаут до 5 минут
  retries: 3
  retry_delay: 30
```

### Проблема 3: Нехватка места на диске

**Симптомы**: Процесс останавливался из-за нехватки места

**Решение**:
- Использовали сжатие данных
- Разбивали архив на части
- Переносили готовые части на другое хранилище

### Проблема 4: Долгие процессы

**Симптомы**: Процесс мог прерваться из-за сетевых сбоев

**Решение**:
```yaml
resume:
  enabled: true
  checkpoint_file: "checkpoint.json"
  checkpoint_interval: 10000  # Сохранять прогресс каждые 10K записей
```

## Лучшие практики

### Планирование

1. **Оценка перед запуском**: Всегда используйте `estimate` перед полной архивацией
2. **Тестовый запуск**: Начните с небольшого объёма (100-1000 записей)
3. **Мониторинг ресурсов**: Следите за использованием диска и памяти

### Конфигурация

1. **Консервативные лимиты**: Начинайте с меньших значений rate limiting
2. **Обработка ошибок**: Настройте retry и обработку ошибок
3. **Логирование**: Включите подробное логирование

### Мониторинг

1. **Прогресс**: Регулярно проверяйте логи и прогресс
2. **Ошибки**: Отслеживайте частоту ошибок
3. **Производительность**: Мониторьте скорость загрузки

### Хранение

1. **Сжатие**: Используйте сжатие для экономии места
2. **Разбиение**: Разбивайте большие архивы на части
3. **Проверка**: Проверяйте целостность после архивации

## Автоматизация

### Скрипт для регулярной архивации

```bash
#!/bin/bash
# regular-api-backup.sh

PROJECT="budget-portal"
DATE=$(date +%Y%m%d)

# Проверка новых данных
NEW_RECORDS=$(apibackuper estimate incremental -p $PROJECT --since $(date -d "7 days ago" +%Y-%m-%d) | grep "records" | awk '{print $2}')

if [ "$NEW_RECORDS" -gt 0 ]; then
  echo "Найдено $NEW_RECORDS новых записей. Запускаем архивацию..."
  
  # Инкрементальная архивация
  apibackuper run incremental -p $PROJECT --since $(date -d "7 days ago" +%Y-%m-%d)
  
  # Экспорт новых данных
  apibackuper export "budget_incremental_$DATE.jsonl" -p $PROJECT
  
  echo "Архивация завершена"
else
  echo "Новых записей не найдено"
fi
```

### Мониторинг через cron

```bash
# Добавить в crontab для еженедельного запуска
0 2 * * 0 /path/to/regular-api-backup.sh >> /var/log/api-backup.log 2>&1
```

## Выводы

1. **Масштабируемость**: APIBackuper успешно справляется с миллионами записей
2. **Надёжность**: Правильная настройка обеспечивает стабильную работу
3. **Гибкость**: Конфигурация позволяет адаптироваться к разным API
4. **Автоматизация**: Процесс можно полностью автоматизировать

## Связанные материалы

- [APIBackuper документация](/kb/instruments/downloaded-data/apibackuper)
- [Конфигурации APIBackuper](/kb/instruments/downloaded-data/apibackuper-configs)
- [Создание кастомных workflow](/kb/guides/custom-workflows)
- [Архивация комплексных ресурсов](/kb/case-studies/multi-tool-archiving)
