---
sidebar_position: 5
---

# Архивация комплексных ресурсов: использование нескольких инструментов

Реальный пример архивации государственного веб-ресурса, который использует несколько технологий и требует комбинации различных инструментов Ruarxive.

## Задача

Необходимо было заархивировать комплексный веб-ресурс государственного органа, который состоял из:

*   **Основной сайт на WordPress** — публичные страницы и статьи
*   **SharePoint портал** — внутренние документы и файлы
*   **REST API** — структурированные данные и датасеты
*   **Статические файлы** — PDF документы, отчёты, медиа-файлы

Каждый компонент требовал своего подхода к архивации.

## Вызовы

### Технические сложности

1. **Разные технологии**: Каждый компонент использовал свою технологию
2. **Разные форматы данных**: HTML, JSON, файлы, базы данных
3. **Связи между компонентами**: Нужно было сохранить связи между данными
4. **Объём данных**: Несколько десятков гигабайт информации
5. **Ограничения API**: Rate limiting и ограничения доступа

### Временные ограничения

*   Ресурс планировалось закрыть через месяц
*   Необходимо было успеть заархивировать всё до закрытия
*   Некоторые компоненты становились недоступными раньше других

## Решение: комбинированный подход

### Этап 1: Анализ структуры

Сначала был проведён анализ структуры ресурса:

```bash
# Проверка доступности WordPress API
curl https://example.gov.ru/wp-json/wp/v2/

# Проверка SharePoint API
spcrawler ping --url https://example.gov.ru

# Анализ доступных эндпоинтов API
curl https://api.example.gov.ru/v1/endpoints
```

### Этап 2: Архивация WordPress контента

Использовали **wparc** для получения структурированных данных:

```bash
# Архивация WordPress через API
wparc https://example.gov.ru/ --output ./wordpress-archive

# Результат:
# - posts.json - все статьи
# - pages.json - все страницы
# - media/ - изображения и медиа-файлы
# - comments.json - комментарии
```

**Преимущества wparc**:
*   Чистые данные без HTML-разметки
*   Сохранение метаданных (даты, авторы, теги)
*   Быстрая загрузка через API

### Этап 3: Архивация SharePoint

Использовали **spcrawler** для дампа SharePoint:

```bash
# Сначала проверили структуру
spcrawler walk --url https://example.gov.ru

# Затем создали полный дамп
spcrawler dump --url https://example.gov.ru --output ./sharepoint-archive

# Результат:
# - Метаданные всех списков и библиотек
# - Все файлы из документных библиотек
# - Структурированные данные в JSON
```

**Особенности**:
*   SharePoint API требует аутентификации
*   Большие файлы скачивались отдельно
*   Сохранялась структура папок

### Этап 4: Архивация REST API

Использовали **apibackuper** для структурированных данных:

```yaml
# config.yaml для apibackuper
project:
  name: example-gov-api

request:
  url: "https://api.example.gov.ru/v1/datasets"
  method: GET
  params:
    page: 1
    per_page: 100
  iterator:
    param: page
    start: 1
    step: 1

rate_limit:
  enabled: true
  requests_per_second: 5

storage:
  type: zip
  file: api_data.zip
```

```bash
# Оценка объёма
apibackuper estimate full -p example-gov-api

# Запуск архивации
apibackuper run full -p example-gov-api
```

**Результат**:
*   Все датасеты из API
*   Метаданные и структура данных
*   Экспорт в JSONL для дальнейшей обработки

### Этап 5: Скачивание статических файлов

Использовали **filegetter** для массового скачивания файлов:

```yaml
# config.yaml для filegetter
patterns:
  - url: "https://example.gov.ru/reports/{year}/report_{id:04d}.pdf"
    range:
      year: [2020, 2021, 2022, 2023]
      id:
        start: 1
        end: 100

output:
  directory: "./static-files"
  create_subdirs: true

download:
  threads: 5
  retries: 3
```

```bash
filegetter --config config.yaml
```

**Результат**:
*   Все PDF отчёты за несколько лет
*   Организованная структура папок
*   Проверка целостности файлов

### Этап 6: Объединение и верификация

После архивации всех компонентов:

```bash
# Создание единого архива
tar -czf example-gov-complete-archive.tar.gz \
  wordpress-archive/ \
  sharepoint-archive/ \
  api_data.zip \
  static-files/

# Проверка целостности
sha256sum example-gov-complete-archive.tar.gz > checksum.txt
```

## Workflow автоматизации

Для координации всех этапов был создан скрипт:

```bash
#!/bin/bash
# archive-complex-resource.sh

SITE_URL="https://example.gov.ru"
OUTPUT_DIR="./archive-$(date +%Y%m%d)"

mkdir -p "$OUTPUT_DIR"

echo "Этап 1: WordPress архивация"
wparc "$SITE_URL" --output "$OUTPUT_DIR/wordpress"

echo "Этап 2: SharePoint архивация"
spcrawler dump --url "$SITE_URL" --output "$OUTPUT_DIR/sharepoint"

echo "Этап 3: API архивация"
apibackuper run full -p example-gov-api
mv example-gov-api/data.zip "$OUTPUT_DIR/api_data.zip"

echo "Этап 4: Статические файлы"
filegetter --config filegetter-config.yaml --output "$OUTPUT_DIR/static"

echo "Этап 5: Создание финального архива"
tar -czf "example-gov-archive-$(date +%Y%m%d).tar.gz" "$OUTPUT_DIR"

echo "Архивация завершена"
```

## Результаты

### Объёмы данных

*   **WordPress**: 2.3 GB (статьи, страницы, медиа)
*   **SharePoint**: 15.7 GB (документы, файлы)
*   **API данные**: 1.1 GB (структурированные датасеты)
*   **Статические файлы**: 3.2 GB (PDF, документы)
*   **Итого**: ~22.3 GB

### Качество архивации

*   ✅ **Полнота**: Все компоненты заархивированы
*   ✅ **Структура**: Сохранена организация данных
*   ✅ **Метаданные**: Все метаданные сохранены
*   ✅ **Целостность**: Проверены контрольные суммы
*   ✅ **Доступность**: Данные можно использовать для исследований

### Время выполнения

*   **WordPress**: 2 часа
*   **SharePoint**: 8 часов
*   **API**: 4 часа
*   **Статические файлы**: 3 часа
*   **Обработка и упаковка**: 2 часа
*   **Итого**: ~19 часов

## Уроки и выводы

### Что сработало хорошо

1. **Разделение на этапы**: Каждый компонент архивировался отдельно
2. **Параллельная работа**: Некоторые этапы можно было выполнять параллельно
3. **Автоматизация**: Скрипт координировал весь процесс
4. **Проверка на каждом этапе**: Проблемы выявлялись сразу

### Проблемы и решения

1. **Rate limiting API**
   - **Проблема**: API ограничивал количество запросов
   - **Решение**: Настроили rate limiting в apibackuper

2. **Большие файлы в SharePoint**
   - **Проблема**: Некоторые файлы были очень большими
   - **Решение**: Использовали резюме загрузки в spcrawler

3. **Связи между данными**
   - **Проблема**: Нужно было сохранить связи между компонентами
   - **Решение**: Создали индексный файл с перекрёстными ссылками

### Рекомендации

1. **Начинайте с анализа**: Изучите структуру ресурса перед архивацией
2. **Используйте правильный инструмент**: Каждый тип данных требует своего подхода
3. **Автоматизируйте процесс**: Создавайте скрипты для координации
4. **Проверяйте на каждом этапе**: Не ждите окончания всей архивации
5. **Документируйте процесс**: Записывайте, что и как было заархивировано

## Применение подхода

Этот подход можно применять для архивации:

*   Комплексных государственных порталов
*   Корпоративных сайтов с несколькими компонентами
*   Образовательных платформ
*   Медиа-ресурсов с разными типами контента

## Связанные материалы

- [Создание кастомных workflow](/kb/guides/custom-workflows)
- [wparc документация](/kb/instruments/ruarxive-tools/wparc)
- [spcrawler документация](/kb/instruments/downloaded-data/spcrawler)
- [apibackuper документация](/kb/instruments/downloaded-data/apibackuper)
- [filegetter документация](/kb/instruments/ruarxive-tools/filegetter)
